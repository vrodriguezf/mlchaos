{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Motion classification with time series using deep learning\n",
    "\n",
    "> Classify between chaotic, rotational and librational motion using the evolution of the coordinates x and y in the Poincare map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "ISCOLAB = 'google.colab' in sys.modules\n",
    "if ISCOLAB:\n",
    "    !pip install git+https://github.com/fastai/fastcore.git@master -q\n",
    "    !pip install git+https://github.com/fastai/fastai2.git@master -q\n",
    "    !pip install git+https://github.com/ai-fast-track/timeseries.git -q\n",
    "    !pip install wandb -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.basics import *\n",
    "from timeseries.all import *\n",
    "from mlchaos.all import *\n",
    "import wandb\n",
    "from fastai.callback.wandb import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = Path.home()\n",
    "ds_path = base_path/'data/fli_rotnum/model_1'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook parameters \n",
    "Put here everything that could be needed if this notebook was called from outside, as an script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "use_wandb = True # Whether to use or not wandb for experiment tracking\n",
    "wandb_group = None # Whether to group this run in a wandb group\n",
    "resampling_period = None # * Natural number. Set to None if no resampling is desired\n",
    "labelling_method = \"birkhoff_avg\" # [fli, freq_ana, birkhoff_avg, combined]\n",
    "val_dataset = 'eps=0.023' # * The rest of the datasets in fnames will be used for training\n",
    "fnames_poinc_map = [ds_path/'eps=0.021/Poincare.plt', \n",
    "                    ds_path/'eps=0.022/Poincare.plt', \n",
    "                    ds_path/'eps=0.023/Poincare.plt', \n",
    "                    ds_path/'eps=0.024/Poincare.plt', \n",
    "                    ds_path/'eps=0.025/Poincare.plt']\n",
    "fnames_index = [ds_path/'eps=0.021/index_fli_rotnum_birav_v2.plt', \n",
    "               ds_path/'eps=0.022/index_fli_rotnum_birav_v2.plt',\n",
    "               ds_path/'eps=0.023/index_fli_rotnum_birav_v2.plt',\n",
    "               ds_path/'eps=0.024/index_fli_rotnum_birav_v2.plt',\n",
    "               ds_path/'eps=0.025/index_fli_rotnum_birav_v2.plt']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Syncing with Weights & Biases\n",
    "\n",
    "For large experiments, when this notebooks is called from outside with different parameters, it is important to use wandb to track the results and config of this notebook and to track and aggregate the results online afterwards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not use_wandb:\n",
    "    os.environ['WANDB_MODE'] = 'dryrun' # run offline\n",
    "    wandb.init(anonymous='allow', allow_val_change=True)\n",
    "else:\n",
    "    os.environ['WANDB_MODE'] = 'run'\n",
    "    wandb.init(entity=\"vrodriguezf\", project='mlchaos', group=wandb_group)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data must be loaded as a `TSDataChaos` object, created from a pair (or a sequence of pairs) of files:\n",
    "1. The first file of the pair contains the data of the Poincare map\n",
    "2. The second file of the pair contains the index of the motion (0.0, 1.0 or 2.0). The column that contains the index is given as a parameter of this notebook\n",
    "\n",
    "Depending on the labelling method, a different column from the index file will be used as `y` on the data. `x` will always be the time series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.config.fnames_poinc_map = fnames_poinc_map\n",
    "wandb.config.fnames_index = fnames_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fnames = L(fnames_poinc_map).zipwith(fnames_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.config.labelling_method = labelling_method\n",
    "index_cols = {\n",
    "    \"fli\": 4,\n",
    "    \"freq_ana\": 5,\n",
    "    \"birkhoff_avg\": 6,\n",
    "    \"combined\": [4, 5, 6]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.config.fnames = fnames\n",
    "data = TSDataChaos.from_poincare_and_index_files(fnames, \n",
    "                                                 index_col=index_cols[labelling_method])\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the distribution of classes in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(data.y, return_counts=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The class -1 represents uncertainty in the FLI-based classication. If desired, we can remove that class from the dataset, by changing the flag `add_uncertainty_class` in the cell below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.config.add_uncertainty_class = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Move this to a function\n",
    "if not wandb.config.add_uncertainty_class:\n",
    "    data.x = data.x[data.y != -1]\n",
    "    data.ds = data.ds[data.y != -1]\n",
    "    data.y = data.y[data.y != -1]\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data can be resampled to a lower frequency if desired. The reason why this is interesting is because being able to produce an accurate classification without the need of many data points per time series makes this procedure applicable to observational data. As an example, choosing `resampling_period` equals to 2 means that the time series will be resampled in a way that we take one point every 2 points of the original data.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.config.resampling_period = resampling_period"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if wandb.config.resampling_period is not None:\n",
    "    data.x = data.x[:,:,::wandb.config.resampling_period]\n",
    "    print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use a dictionary to establish how to label each index to an actual human-readable label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lbl_dict = dict([\n",
    "    ('-1.0', 'uncertain'),\n",
    "    ('0.0', 'chaotic'),   \n",
    "    ('1.0', 'rotational'),   \n",
    "    ('2.0', 'librational')]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting the data into training and validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One easy option to split data is to use 80% random items from the data as training and 20% as validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "splits = RandomSplitter()(range_of(data.get_items()))\n",
    "#splits = (range_of(data.get_items()), range_of(data.get_items())) # Valid = train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another way to split the data, which is more sensible if we want to ensure that the knowledge in the model can be transferred to unknown dynamic models, is to ensure that motions in the validation set are not included in the training set. We can do this by ensuring that the motions of the validation set belong to a specific dynamic model, and the rest of dynamics will be used for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.config.val_ds = ifnone(wandb.config.get('val_ds'), val_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_idxs = np.where(data.ds == data.dsname.index(wandb.config.val_ds))[0]\n",
    "val_idxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "splits = IndexSplitter(val_idxs)(data.get_items())\n",
    "splits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Creating a `Datasets` object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfms = [[ItemGetter(0), ToTensorTS(), ToTensorMotion()], # x transforms\n",
    "        [ItemGetter(1), lbl_dict.get, Categorize()]] # y transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = Datasets(items=data.get_items(), tfms=tfms, splits=splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.config['ds.train.__len__()'] = ds.train.__len__()\n",
    "wandb.config['ds.valid.__len__()'] = ds.valid.__len__()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show one element of the dataset. By default, the `show` method will display the item as a Poincare map."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = show_at(ds, 1003, figsize=(1,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It can be useful to visualize the motions with he same y limits. We take them from the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ylim = [data.x[:, 1].min(), data.x[:, 1].max()]\n",
    "ylim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = show_at(ds, 1003, figsize=(1,1), ylim=ylim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a `Dataloaders` object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs = 512                            \n",
    "# Normalize at batch time\n",
    "tfm_norm = Standardize(scale_subtype = 'per_sample')\n",
    "batch_tfms = []\n",
    "\n",
    "dls = ds.dataloaders(bs=bs, val_bs=bs * 2, after_batch=batch_tfms, num_workers=0, device=default_device()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dls.show_batch(max_n=9, ylim=ylim, return_fig=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xb, yb = dls.one_batch()\n",
    "print(xb[0].mean(axis=1), xb[0].std(axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of channels\n",
    "c_in = get_n_channels(dls.train) # data.n_channels\n",
    "# Number of classes\n",
    "c_out= dls.c \n",
    "c_in,c_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = inception_time(c_in, c_out).to(device=default_device())\n",
    "# model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a Learner object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# opt_func = partial(Adam, lr=3e-3, wd=0.01)\n",
    "#Or use Ranger\n",
    "def opt_func(p, lr=slice(3e-3)): return Lookahead(RAdam(p, lr=lr, mom=0.95, wd=0.01)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Learner    \n",
    "loss_func = LabelSmoothingCrossEntropy() \n",
    "learn = Learner(dls, model, opt_func=opt_func, \n",
    "                loss_func=loss_func, metrics=accuracy)\n",
    "\n",
    "# print(learn.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_min, lr_steep = learn.lr_find()\n",
    "lr_min, lr_steep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.fit_one_cycle(10, lr_max=1e-3, cbs=WandbCallback(log_preds=False))\n",
    "learn.recorder.plot_loss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Showing results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results for random items in the validation set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.show_results(max_n=9, ylim=ylim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interp = ClassificationInterpretationAugmented.from_learner(learn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interp.plot_confusion_matrix()\n",
    "plt.savefig('./tmp.png')\n",
    "wandb.log({'confusion_matrix': wandb.Image('./tmp.png')})\n",
    "!rm tmp.png"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Smallest errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interp.plot_top_losses(k=9, figsize=(15, 15), largest=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Largest errores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interp.plot_top_losses(k=9, figsize=(15, 15), nrows=5, largest=True, ylim=ylim, ncols=3)\n",
    "plt.savefig('./tmp.png')\n",
    "wandb.log({'largest_errors': wandb.Image('./tmp.png')})\n",
    "!rm tmp.png"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Specific types of errors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is also interesting to display separately errors of different type. There are a number of `c*(c-1)` possible type of errors, where `c` is the number of classes in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(interp.top_losses(k=3, largest=False, predicted='chaotic', actual='chaotic'))\n",
    "print(interp.top_losses(k=3, largest=False, predicted='rotational', actual='rotational'))\n",
    "print(interp.top_losses(k=3, largest=False, predicted='librational', actual='librational'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = 'chaotic'\n",
    "actual = 'chaotic'\n",
    "n_errors = interp.confusion_matrix()[interp.vocab.o2i[predicted], \n",
    "                                     interp.vocab.o2i[actual]]\n",
    "interp.plot_top_losses(k=min(n_errors, 9), figsize=(15, 15), \n",
    "                       largest=True, ylim=ylim, \n",
    "                       predicted=predicted, \n",
    "                       actual=actual)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overview of the results through a labelled initial condition map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show each motion in the validation set in a Poincare map showing just its initial point (x0, y0), coloured by the dynamical index associated to it. We will plot two figures, one with the dynamical index given by the ML model (predictions) and one with the true dynamic indices given by the `labelling_method`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ml_map = show_labelled_ic_map(interp.inputs[0], \n",
    "                    [dls.tfms[1].decode(y) for y in interp.decoded], \n",
    "                    legend=None,\n",
    "                    palette=dict(\n",
    "                        chaotic='yellow', \n",
    "                        librational='blue', \n",
    "                        rotational='magenta'))\n",
    "\n",
    "wandb.log({f'ML map ({wandb.config.val_ds})': ml_map.figure})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_map = show_labelled_ic_map(interp.inputs[0], \n",
    "                    [dls.tfms[1].decode(y) for y in interp.targs], \n",
    "                    legend=None,\n",
    "                    palette=dict(\n",
    "                        chaotic='yellow', \n",
    "                        librational='blue', \n",
    "                        rotational='magenta'))\n",
    "\n",
    "wandb.log({f'True map ({wandb.config.val_ds})': true_map.figure})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explaining the model predictions through class activation maps (TODO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the function `show_cam` from the library `timeseries` to plot the Class Activation Maps (CAMs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "?show_cam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need a function `i2o` that, given an encoded y, i.e, an instance of `TensorCategory`, return the label associated to that index, i.e., we need to decode the labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def i2o(y):\n",
    "    return dls.tfms[1].decode(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(i2o(TensorCategory(0)))\n",
    "print(i2o(TensorCategory(1)))\n",
    "print(i2o(TensorCategory(2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference on a new test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's see how the trained behaves with a completely unseen set of data. First we have to load the Poincare maps that comprise the test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the predicted labels using the library `fastinference`. It is faster than the original `fastai` implementation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#old_get_preds = learn.get_preds\n",
    "#from fastinference.inference import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "test_base_path = base_path/'data/eps=0.02_pert_pend_model_2'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "fnames_test = (test_base_path/'Poincare200pt.plt', \n",
    "               test_base_path/'index200pt.plt')\n",
    "test_data = TSDataChaos.from_poincare_and_index_files(fnames_test)\n",
    "test_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "if not wandb.config.add_uncertainty_class:\n",
    "    test_data.x = test_data.x[test_data.y != -1]\n",
    "    test_data.y = test_data.y[test_data.y != -1]\n",
    "test_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a dataloader from the test items previously loaded. This will apply all the transformations used in our training and validation set (e.g. normalization) to the test data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "test_dl = learn.dls.test_dl(test_data.get_items(), with_labels=True)\n",
    "test_dl.n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "preds, targs, preds_decoded = learn.get_preds(dl=test_dl, with_decoded=True)\n",
    "preds.__len__()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute metrics (validation loss and accuracy) on the test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "test_metrics = learn.validate(dl=test_dl)\n",
    "test_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show also trhe confusion matrix for this test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# learn.get_preds = old_get_preds\n",
    "interp_test = ClassificationInterpretationAugmented.from_learner(learn, \n",
    "                                                                 dl=test_dl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "interp_test.plot_confusion_matrix(\n",
    "    title=f'Confusion matrix (accuracy={round(test_metrics[1], 2)})',\n",
    ")\n",
    "plt.savefig(test_base_path/f'confusion_matrix_{test_base_path.name}.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show the FLI plot to get a sense of the quality of the predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ml_map = show_fli_plot(test_data.x, \n",
    "                    [dls.tfms[1].decode(y) for y in preds_decoded], \n",
    "                    legend=None,\n",
    "                    palette=dict(\n",
    "                        chaotic='yellow', \n",
    "                        librational='blue', \n",
    "                        rotational='magenta'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ml_map.figure.savefig(test_base_path/f'ml_map_{test_base_path.name}.eps', \n",
    "                      transparent=True, format='.eps')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now show the same plot for the true labels given by the FLI-based classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "true_map = show_fli_plot(test_data.x, \n",
    "                    [dls.tfms[1].decode(y) for y in targs], \n",
    "                    legend=None,\n",
    "                    palette=dict(\n",
    "                        chaotic='yellow', \n",
    "                        librational='blue', \n",
    "                        rotational='magenta'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "true_map.figure.savefig(test_base_path/f'true_map_{test_base_path.name}.eps', \n",
    "                        transparent=True, format='.eps')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
